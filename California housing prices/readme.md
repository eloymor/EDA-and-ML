# California Housing Price Prediction

This project provides a comprehensive solution for predicting California housing prices.
Data is extracted from the [California Housing Dataset](https://www.kaggle.com/camnugent/california-housing-prices) on Kaggle.
The project includes:
* Extract, Transform, Load (ETL) pipeline.
* Machine learning pipeline with hyperparameter optimization using Optuna and XGBoost.
* FastAPI application to serve predictions, manage model training and ETL via an API.
* Command line interface (CLI) for ETL and model training.
* Data exploration and visualization.
* Data management using SQLite.

## Features

*   **ETL Pipeline**: Processes raw housing data, handles missing values, caps outliers, performs geographical clustering using DBSCAN, and stores the cleaned data in an SQLite database.
*   **Model Training**: Trains an XGBoost regression model to predict `median_house_value`.
    *   Utilizes `Optuna` for efficient hyperparameter optimization.
    *   Supports both Command Line Interface (CLI) and API-driven training.
*   **Prediction API**: A FastAPI application that provides:
    *   Endpoint for real-time housing price predictions.
    *   Endpoint to trigger model retraining.
    *   Endpoint to retrieve raw data.
    *   Endpoint to retrieve processed data.
    *   Endpoint to check API health.
*   **Data Management**: Uses SQLite for storing processed data.
*  **Data Exploration and Visualization**: Provides data exploration and visualization using Plotly and pandas.

## Project Structure

*   `etl.py`: Contains the ETL logic for data extraction, transformation, and loading into an SQLite database.
*   `train_model.py`: Handles the machine learning model training process, including data loading, splitting, hyperparameter optimization, and model saving.
*   `main.py`: Implements the FastAPI application, exposing endpoints for predictions, training, and data retrieval.
*   `data/`: Directory for raw input data and auxiliary files like `proximity_mapping.json`.
*   `models/`: Directory where the trained XGBoost model (`xgboost_model.json`) will be saved.
*   `etl_data.db`: The SQLite database file generated by the ETL process, containing processed housing data.
*   `EDA.ipynb`: Jupyter notebook for data exploration and visualization.

## Setup and Installation

This project requires Python 3.13.5. We use `uv` as the package manager.

1.  **Clone the repository**:

    ```bash
    git clone https://github.com/eloymor/EDA-and-ML.git
    cd EDA-and-ML/California housing prices/
    ```

2.  **Install dependencies**:

    First, ensure you have `uv` installed. If not, you can install it via `pip`:
    ```bash
    pip install uv
    ```
    Initialize the virtual environment:
    ```bash
    uv init
    ```
    At this point, should detect the `project.toml` file in the project root, and automatically add the packages.
    If not, install the project dependencies manually:
    ```bash
    uv add fastapi pydantic optuna uvicorn, xgboost, sqlalchemy, pandas, plotly, scikit-learn
    ```


## Usage

### 1. Data ETL Process

Before training the model, you need to process your raw data. Place your raw housing data file (e.g., `housing.csv`) in the `data/` directory.

Run the ETL script from the project root:

_Note: all entry files must inside the `data/` directory._

```bash
python etl.py housing.csv
```

This will create an SQLite database with the table. `etl_data.db``processed_data`


### 2. Model Training
You can train the model either via CLI or the API.
#### CLI Training
To train the model using the command line:
```bash
python train_model.py
```

The trained model will be saved as in the `models/` directory. `xgboost_model.json`

#### API Training
Once the FastAPI application is running (see below), you can trigger model training via a POST request:
```bash
curl -X POST "http://localhost:8000/train"

```
### 3. Running the Prediction API
To start the FastAPI application:
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```
Or run the `main.py` file directly:
```bash
cd api/
uv run main.py
```

The API will be accessible at `http://localhost:8000`.
### API Endpoints
- **(GET)`/health`**: Checks the health of the API and model loading status. Example: `http://localhost:8000/health`
- **(GET)`/raw_data`**: Retrieves all raw data (before ETL) stored in the database. Example: `http://localhost:8000/raw_data`
- **(GET)`/processed_data`**: Retrieves all processed data (after ETL) stored in the database. Example: `http://localhost:8000/processed_data`
- **(POST)`/predict`**: Predicts the median house value based on input features. Request Body Example:
```JSON
{
      "longitude": -122.23,
      "latitude": 37.88,
      "housing_median_age": 41.0,
      "total_rooms": 880.0,
      "total_bedrooms": 129.0,
      "population": 322.0,
      "households": 126.0,
      "median_income": 8.3252,
      "ocean_proximity": "NEAR BAY"
    }

```

Example: `POST http://localhost:8000/predict`
- **(GET)`/train`**: Triggers the model retraining process. Example: `http://localhost:8000/train` 
- **(POST)** You can also specify the database and the table name `http://localhost:8000/train/my_custom_db.db/my_table`
- **(POST)** `/etl/filename.csv` Triggers the ETL pipeline. Example: http://localhost:8000/etl/filename.csv
- **(POST)** `/etl/filename.csv/db_name/table_name` Triggers the ETL pipeline with a custom database and table. Example: http://localhost:8000/etl/filename.csv/my_custom_db/my_table
- **(GET)`/docs`**: Opens the Swagger UI documentation page. Example: `http://localhost:8000/docs`
